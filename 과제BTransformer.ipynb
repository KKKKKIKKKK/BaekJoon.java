{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KKKKKIKKKK/BaekJoon.java/blob/master/%EA%B3%BC%EC%A0%9CBTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymxatB5WYxlL"
      },
      "source": [
        "# Transformer 실습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1X7RM2du1zcr",
        "outputId": "6e201dd8-2c4d-42ca-95e9-8d5802f2ef21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wypJuWzUlbLm",
        "outputId": "3e3a5975-1aa8-4186-a868-12b7ea66b57d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  IMDB 영화 리뷰 데이터셋을 로드하고 처리\n",
        "- 목적:\n",
        "IMDB 영화 리뷰 데이터셋을 로드하고, 이를 BERT 모델에 입력할 수 있는 형태로 전처리하는 단계\n",
        "- 이는 감성 분석(긍정/부정 분류) 같은 자연어 처리 작업을 위한 준비"
      ],
      "metadata": {
        "id": "_2cK_q4OnQ2f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HOdhoBVA1zcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428b3bcc-0853-4483-cb97-f3edc1a36617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    labels.append(row['label'])\n",
        "    texts.append(row['text'])\n",
        "\n",
        "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-FshZcTZBQ2"
      },
      "source": [
        "## Self-attention\n",
        "\n",
        "이번에는 self-attention을 구현해보겠습니다.\n",
        "Self-attention은 shape이 (B, S, D)인 embedding이 들어왔을 때 attention을 적용하여 새로운 representation을 만들어내는 module입니다.\n",
        "여기서 B는 batch size, S는 sequence length, D는 embedding 차원입니다.\n",
        "구현은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MBlMVMZcRAxv"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, input_dim, d_model):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.wq = nn.Linear(input_dim, d_model)\n",
        "    self.wk = nn.Linear(input_dim, d_model)\n",
        "    self.wv = nn.Linear(input_dim, d_model)\n",
        "    self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
        "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
        "    score = score / sqrt(self.d_model)\n",
        "\n",
        "    if mask is not None:\n",
        "      score = score + (mask * -1e9)\n",
        "\n",
        "    score = self.softmax(score)\n",
        "    result = torch.matmul(score, v)\n",
        "    result = self.dense(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TransformerLayer를 구현\n",
        "\n",
        "1. TransformerLayer 클래스 구조\n",
        "2. Self-Attention 구현\n",
        "3. Feed-Forward Network (FFN) 구현\n",
        "4. Forward 패스\n",
        "5. Mask 처리\n",
        "\n"
      ],
      "metadata": {
        "id": "W6CosvsDnvmh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VZHPCn9AS5Gp"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "\n",
        "    self.sa = SelfAttention(input_dim, d_model)\n",
        "    self.ffn = nn.Sequential(\n",
        "      nn.Linear(d_model, dff),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(dff, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sa(x, mask)\n",
        "    x = self.ffn(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3VYrqTJagS1"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "$$\n",
        "\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n",
        "$$\n",
        "\n",
        "이를 Numpy로 구현하여 PyTorch tensor로 변환한 모습은 다음과 같습니다:\n",
        "\n",
        "- max_len = 400, d_model = 256으로 설정하여 positional encoding을 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Uf_jMQWDUR79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59dc7d7-3be7-4a48-b9db-a6657355e140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 400, 256])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "\n",
        "max_len = 400\n",
        "print(positional_encoding(max_len, 256).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5unoDcBva3eN"
      },
      "source": [
        "## Transformer 기반의 텍스트 분류기 모델을 구현\n",
        "- 임베딩 층, 위치 인코딩, 여러 개의 Transformer 층, 그리고 분류 층으로 구성\n",
        "- 초기화 (init 메소드)\n",
        "- 순전파 (forward 메소드)\n",
        "- 이 TextClassifier 모델은 Transformer 아키텍처를 기반으로 한 텍스트 분류기입니다. 입력 텍스트를 토큰화하고, 임베딩하며, 위치 정보를 추가한 후, 여러 Transformer 층을 통과시켜 특징을 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8MaiCGh8TsDH"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.dff = dff\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n",
        "    self.classification = nn.Linear(d_model, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :]\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = x * sqrt(self.d_model)\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, 0]\n",
        "    x = self.classification(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier(len(tokenizer), 32, 2, 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDq05OlAb2lB"
      },
      "source": [
        "## 텍스트 분류 모델의 학습을 위한 준비 단계\n",
        "- 설명:\n",
        "이 코드는 텍스트 분류 모델의 학습을 위한 핵심 요소들을 설정합니다. Adam 옵티마이저를 사용하여 모델 파라미터를 효율적으로 업데이트하고, BCEWithLogitsLoss를 통해 이진 분류 문제에 적합한 손실을 계산합니다. 모델을 GPU로 이동시켜 학습 속도를 향상시키며, 0.001의 학습률로 학습을 진행할 준비를 합니다. 이러한 설정은 모델이 IMDB 리뷰 데이터셋에서 감성 분석(긍정/부정 분류)을 효과적으로 학습할 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YHVVsWBPQmnv"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to('cuda')\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델의 정확도를 평가하는 함수를 정의"
      ],
      "metadata": {
        "id": "yJno9IFdp6_X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "r88BALxO1zc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    # preds = torch.argmax(preds, dim=-1)\n",
        "    preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 분류 모델의 학습 과정을 구현"
      ],
      "metadata": {
        "id": "zsESMU_iqCNy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al_b56TYRILq"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda').float()\n",
        "\n",
        "    preds = model(inputs)[..., 0]\n",
        "    loss = loss_fn(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과제 B. 주어진 문장에서 나올 다음 단어를 예측하는 모델 구현"
      ],
      "metadata": {
        "id": "SSUStbrUvbuh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NAXB6GgIQy1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc63a321-9700-4d20-9198-584ca4caff96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 20, 10000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Multi-Head Attention Module (멀티헤드 어텐션 모듈)\n",
        "# 여러 개의 attention head를 사용하여 attention을 병렬로 수행하는 부분\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = embed_dim // num_heads  # 각 head의 차원 계산\n",
        "\n",
        "        # head_dim이 num_heads로 나누어떨어지는지 확인하는 assert문\n",
        "        assert self.head_dim * num_heads == embed_dim, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "        # query, key, value를 위한 선형 변환 정의 (Wq, Wk, Wv)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Output projection (출력 투영)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "\n",
        "        # Query, Key, Value 계산 (각각 q_proj, k_proj, v_proj를 사용)\n",
        "        Q = self.q_proj(x)\n",
        "        K = self.k_proj(x)\n",
        "        V = self.v_proj(x)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len, head_dim) 형태로 변환\n",
        "        Q = Q.view(batch_size, self.num_heads, seq_len, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, self.num_heads, seq_len, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, self.num_heads, seq_len, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled Dot-Product Attention (QK^T / sqrt(d_k)) 계산\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # 마스크가 있을 경우 마스크 적용 (masking된 위치에는 매우 작은 값으로 처리)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # 소프트맥스를 통해 어텐션 가중치 계산\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attn_weights, V)\n",
        "\n",
        "        # Multi-head attention 결과를 다시 원래 차원으로 되돌림 (원래 형태로 변환)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
        "        return self.out_proj(context)\n",
        "\n",
        "# Feed Forward Network Module (피드포워드 네트워크 모듈)\n",
        "# 트랜스포머에서 각 attention 뒤에 나오는 2층의 피드포워드 네트워크\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)  # 첫 번째 선형 변환\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)  # 두 번째 선형 변환\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 활성화 함수(ReLU)를 적용 후 두 번째 선형 변환\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "# Transformer Layer with Residual Connections, Dropout, and Layer Normalization\n",
        "# 트랜스포머의 기본 층을 구현, 여기서 residual connection, dropout, layer normalization 적용\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout_prob=0.1):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(embed_dim, num_heads)  # Multi-Head Attention 모듈 사용\n",
        "        self.ffn = FeedForwardNetwork(embed_dim, hidden_dim)  # 피드포워드 네트워크 모듈 사용\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)  # 첫 번째 레이어 노멀라이제이션\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)  # 두 번째 레이어 노멀라이제이션\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # 드롭아웃 적용\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Multi-Head Attention 적용 후 Residual Connection과 Layer Normalization\n",
        "        attn_output = self.mha(x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))  # residual connection: x + attn_output\n",
        "\n",
        "        # Feed-Forward Network 적용 후 Residual Connection과 Layer Normalization\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))  # residual connection: x + ffn_output\n",
        "\n",
        "        return x\n",
        "\n",
        "# Full Transformer model with 5 layers and 4 attention heads\n",
        "# 트랜스포머 전체 모델을 5개의 층과 4개의 어텐션 헤드로 구성\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim, num_layers, vocab_size, dropout_prob=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # 입력을 임베딩하는 층\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerLayer(embed_dim, num_heads, hidden_dim, dropout_prob) for _ in range(num_layers)  # 5개의 TransformerLayer로 구성\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)  # 최종 출력 층\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # 입력에 대해 임베딩 적용\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # 각 트랜스포머 층을 차례대로 통과\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # 마지막에 분류기를 적용하여 단어 예측\n",
        "        return self.fc_out(x)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 32\n",
        "    seq_len = 20\n",
        "    vocab_size = 10000\n",
        "    embed_dim = 512\n",
        "    num_heads = 4\n",
        "    hidden_dim = 2048\n",
        "    num_layers = 5\n",
        "\n",
        "    model = Transformer(embed_dim, num_heads, hidden_dim, num_layers, vocab_size)\n",
        "    input_data = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "    output = model(input_data)\n",
        "    print(output.shape)  # Expected shape: [batch_size, seq_len, vocab_size]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sMDz6wM5xCAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}